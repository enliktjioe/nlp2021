{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnanpnNgx6wj"
   },
   "source": [
    "# Lab 1: NLP Basics\n",
    "## Text Preprocessing\n",
    "Text preprocessing is, probably, one of the least pleasant yet one of the most important steps of a natural language processing (NLP) pipelines. This step determines how your NLP algorithms are going to see the data. If your preprocessing breaks, the whole model can break or, what is even worse, keep silent and give incorrect results.\n",
    "\n",
    "Text preprocessing can be devided into three main parts:\n",
    "* Tokenization\n",
    "* Normalization\n",
    "* Noise reduction\n",
    "\n",
    "The parts are not necessarily applied in that particular order. Sometimes, before tokenization the noise reduction should be performed. In other cases, the some steps can be repeated several times.\n",
    "\n",
    "\n",
    "In this lab we will be using [Python's Natural Language ToolKit (NLTK)](https://www.nltk.org/) and [spaCy](https://spacy.io/usage/spacy-101). Click the previous links to read more about them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ztDD4dWwnkCD"
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize, pos_tag\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# If you don't have the model installed run \"python -m spacy download en_core_web_sm\"\n",
    "# in the console and restart the python kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1sdJQ0cIng75",
    "outputId": "90e9c156-4456-4501-8fde-e2d881087fd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell to install all the necessary files for NLTK\n",
    "nltk.download('stopwords') # Download stopwords \n",
    "nltk.download('wordnet') # Download WordNet \n",
    "nltk.download('punkt') # Download punkt tokenizer models\n",
    "nltk.download('averaged_perceptron_tagger') # Download POS tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xR-6OGGu0sYW"
   },
   "source": [
    "## Tokenization\n",
    "Tokenization may be defined as the process of splitting the text into smaller parts called tokens, and is considered a crucial step in NLP. We can highlight word segmentation and sentence segmentation. Depending on the task, you might need to use only word segmentation, for other tasks, you might want to have both sentences and words.\n",
    "\n",
    "As the names suggest, word segmentation is dividing the raw text sequence into words and sentence segmentation is dividing the text into sentences.\n",
    "\n",
    "Imagine that we need to parse the some sentences  from the Wikipedia article about Coffee. We have the following raw text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O23IHMF00u_C",
    "outputId": "a19b0c66-3af2-4648-ce70-244b65da56ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coffee is a brewed drink prepared from roasted coffee beans, the seeds of berries from certain Coffea species. When coffee berries turn from green to bright red in color – indicating ripeness – they are picked, processed, and dried. By the 16th century, the drink had reached the rest of the Middle East and North Africa, later spreading to Europe.\n"
     ]
    }
   ],
   "source": [
    "raw_text = \"Coffee is a brewed drink prepared from roasted coffee beans, the seeds of berries from certain Coffea species. When coffee berries turn from green to bright red in color – indicating ripeness – they are picked, processed, and dried. \" + \\\n",
    "           \"By the 16th century, the drink had reached the rest of the Middle East and North Africa, later spreading to Europe.\"\n",
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cw50y6aG0ztv"
   },
   "source": [
    "A simple approach is to define a subset of characters as whitespace, and then split the text on these tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RoC3_XAS0zPl",
    "outputId": "6239ddc2-583c-465d-f035-8536ff0ef072"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Coffee', 'is', 'a', 'brewed', 'drink', 'prepared', 'from', 'roasted', 'coffee', 'beans,', 'the', 'seeds', 'of', 'berries', 'from', 'certain', 'Coffea', 'species.', 'When', 'coffee', 'berries', 'turn', 'from', 'green', 'to', 'bright', 'red', 'in', 'color', '–', 'indicating', 'ripeness', '–', 'they', 'are', 'picked,', 'processed,', 'and', 'dried.', 'By', 'the', '16th', 'century,', 'the', 'drink', 'had', 'reached', 'the', 'rest', 'of', 'the', 'Middle', 'East', 'and', 'North', 'Africa,', 'later', 'spreading', 'to', 'Europe.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokens = raw_text.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaFIhcpa1FJZ"
   },
   "source": [
    "But already here, we can see the problem with the tokens like '*dried*.' and '*Africa*,.'. In our case, the dot is the part of the token that we definetely don't want. One solution is to strip each token from the punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2VfPndDB0xkm",
    "outputId": "23e4b3bc-ae76-46b4-f8cf-1536122555d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Coffee', 'is', 'a', 'brewed', 'drink', 'prepared', 'from', 'roasted', 'coffee', 'beans', 'the', 'seeds', 'of', 'berries', 'from', 'certain', 'Coffea', 'species', 'When', 'coffee', 'berries', 'turn', 'from', 'green', 'to', 'bright', 'red', 'in', 'color', '–', 'indicating', 'ripeness', '–', 'they', 'are', 'picked', 'processed', 'and', 'dried', 'By', 'the', '16th', 'century', 'the', 'drink', 'had', 'reached', 'the', 'rest', 'of', 'the', 'Middle', 'East', 'and', 'North', 'Africa', 'later', 'spreading', 'to', 'Europe']\n"
     ]
    }
   ],
   "source": [
    "def whitespace_tokenize(text):\n",
    "    return [token.strip(punctuation) for token in text.split()]\n",
    "    \n",
    "\n",
    "print(whitespace_tokenize(raw_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcwfDE461NPw"
   },
   "source": [
    "Let's say now, that we want to split the text into sentences and then get tokens for each sentence. The simplest way is to split the text by dot first and then get tokens for each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0FzigQpg1MxA",
    "outputId": "760e2723-f4ec-4275-ce20-c523a4467d26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Coffee', 'is', 'a', 'brewed', 'drink', 'prepared', 'from', 'roasted', 'coffee', 'beans', 'the', 'seeds', 'of', 'berries', 'from', 'certain', 'Coffea', 'species'], ['When', 'coffee', 'berries', 'turn', 'from', 'green', 'to', 'bright', 'red', 'in', 'color', '–', 'indicating', 'ripeness', '–', 'they', 'are', 'picked', 'processed', 'and', 'dried'], ['By', 'the', '16th', 'century', 'the', 'drink', 'had', 'reached', 'the', 'rest', 'of', 'the', 'Middle', 'East', 'and', 'North', 'Africa', 'later', 'spreading', 'to', 'Europe']]\n"
     ]
    }
   ],
   "source": [
    "def segment_sents(text):\n",
    "  sents = []\n",
    "  for sent in text.split('.'):\n",
    "    if sent: \n",
    "      sents.append(whitespace_tokenize(sent))\n",
    "  return sents\n",
    "\n",
    "print(segment_sents(raw_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqeUbvID1Xb6"
   },
   "source": [
    "For this example, it worked fine so far. But this task hold many surprises for an unprepared person. Let's see another examples that can cause troubles if using our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aKg5JQ9D1XDK",
    "outputId": "77174f3f-a781-4c42-d451-d0bb602961d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Dr'], ['Ford', 'did', 'not', 'ask', 'Col'], ['Mustard', 'the', 'name', 'of', 'Mr'], [\"Smith's\", 'dog']]\n",
      "[['What', 'is', 'all', 'the', 'fuss', 'about', 'asked', 'Mr'], ['Peters']]\n",
      "[['This', 'full-time', 'student', \"isn't\", 'living', 'in', 'on-campus', 'housing', 'and', \"she's\", 'not', 'wanting', 'to', 'visit', \"Hawai'i\"]]\n"
     ]
    }
   ],
   "source": [
    "difficult_sents = [\n",
    "    \"Dr. Ford did not ask Col. Mustard the name of Mr. Smith's dog.\",\n",
    "    '\"What is all the fuss about?\" asked Mr. Peters.',\n",
    "     \"This full-time student isn't living in on-campus housing, and she's not wanting to visit Hawai'i.\"\n",
    "]\n",
    "\n",
    "for sent in difficult_sents: \n",
    "  print(segment_sents(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBIIUzzR1bqv"
   },
   "source": [
    "Here, we can see that different abbreviations like Dr., Col., Mr. were treated as a sentence end. Also, contractions like let's and she's are in fact two words: is not and she is. However, Smith's can be either Smith is or rather, like in our case, one word showing possession. Finally, we have to decide if full-time and on-campus have one word or two. \n",
    "\n",
    "Luckily, for English, we can use different libraries like NLTK or spaCy which tackle most of these problems. Let's see, how they manage with our sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1uhFS4J11bL3",
    "outputId": "1061f722-5fba-43cd-e00b-78822241fc9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK tokenization:\n",
      "\n",
      "[['Dr.', 'Ford', 'did', 'not', 'ask', 'Col.', 'Mustard', 'the', 'name', 'of', 'Mr.', 'Smith', \"'s\", 'dog', '.']]\n",
      "[['``', 'What', 'is', 'all', 'the', 'fuss', 'about', '?', \"''\"], ['asked', 'Mr.', 'Peters', '.']]\n",
      "[['This', 'full-time', 'student', 'is', \"n't\", 'living', 'in', 'on-campus', 'housing', ',', 'and', 'she', \"'s\", 'not', 'wanting', 'to', 'visit', \"Hawai'i\", '.']]\n"
     ]
    }
   ],
   "source": [
    "print(\"NLTK tokenization:\\n\")\n",
    "for sent in difficult_sents: \n",
    "  print([word_tokenize(s) for s in sent_tokenize(sent)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "muotsnNr1gN6",
    "outputId": "3f7568f9-9f2e-49e5-dbdf-e6f6dd407819"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy tokenization:\n",
      "\n",
      "['Dr.', 'Ford', 'did', 'not', 'ask', 'Col', '.', 'Mustard', 'the', 'name', 'of', 'Mr.', 'Smith', \"'s\", 'dog', '.']\n",
      "['\"', 'What', 'is', 'all', 'the', 'fuss', 'about', '?', '\"', 'asked', 'Mr.', 'Peters', '.']\n",
      "['This', 'full', '-', 'time', 'student', 'is', \"n't\", 'living', 'in', 'on', '-', 'campus', 'housing', ',', 'and', 'she', \"'s\", 'not', 'wanting', 'to', 'visit', \"Hawai'i\", '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Spacy tokenization:\\n\")\n",
    "for sent in difficult_sents:\n",
    "  doc = nlp(sent)\n",
    "  print([token.text for s in doc.sents for token in s])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfc9SAxM1lZQ"
   },
   "source": [
    "As we can see, Spacy is somewhat better for this task. However, this is only that good for English and, probably, most of the European languages. If we take a language where the words are not graphically separated in writing, like Chinese, Thai, or German compound words, we need to choose another approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBo7gfD_1sXI"
   },
   "source": [
    "## Normalization\n",
    "In order to carry out processing on natural language text, we need to perform normalization that mainly involves eliminating punctuation, converting the entire text into lowercase or uppercase, converting numbers into words, expanding abbreviations, canonicalization of text, and so on. \n",
    "\n",
    "We are going to look at the main steps: **stemming** and **lemmatization**. \n",
    "\n",
    "Stemming usually refers to removing endings and prefixes from a word. For example, playing and played are going to be reduced to play after going through the stemmer. It works rather well for English but it can be troublesome for other languages with not complicated morphology. Also, the past tense for run, ran is not going to be changed with stemming and finally is going to be considered a different word. \n",
    "\n",
    "NLTK library includes a stemming package as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xt-5Se8E1lNn",
    "outputId": "bd9390b3-2f4f-4cbb-f0a4-d20d974858cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming with NLTK:\n",
      "\n",
      "['play', 'play', 'play', 'play', 'run', 'ran', 'run', 'run']\n"
     ]
    }
   ],
   "source": [
    "words_to_stem = ['playing', 'played', 'plays', 'play', 'running', 'ran', 'runs', 'run']\n",
    "stemmer = PorterStemmer()\n",
    "print('Stemming with NLTK:\\n')\n",
    "print([stemmer.stem(word) for word in words_to_stem])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cngRv8KO1yWO"
   },
   "source": [
    "To solve the problem with the words that change their roots in different grammarical forms, we should use more complicated method, called lemmatization. Lemmatization is a process wherein the context is used to convert a word to its meaningful base form. It helps in grouping together words that have a common base form and so can\n",
    "be identified as a single item. Now, however, most of the lemmatizers are trained using neural networks.\n",
    "\n",
    "\n",
    "Both NLTK and Spacy have a lemmatization module for English.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gTK71HHJ1xr_",
    "outputId": "1e050beb-b394-4bb5-d78e-8449f0df670e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization with NLTK:\n",
      "\n",
      "playing: playing\n",
      "played: played\n",
      "plays: play\n",
      "play: play\n",
      "running: running\n",
      "ran: ran\n",
      "runs: run\n",
      "run: run\n"
     ]
    }
   ],
   "source": [
    "print('Lemmatization with NLTK:\\n')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for word in words_to_stem:\n",
    "  print(f'{word}: {lemmatizer.lemmatize(word)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFdJ5pYC13kN"
   },
   "source": [
    "We can see immediately that NLTK doesn't give correct lemmas for our words. This is because the NLTK lemmarizer expects to have a part-of-speech (POS) tag for each word, i.e. the information if the word is a noun, a verb, an adjective etc. We can, of course, specify the POS tag for each word but if our corpus is big, it will be tiresome to determine the POS tags by hand. In order to do that, we can use already pretrained POS tagger. We're going to look at POS tagging later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vyblALLs127d",
    "outputId": "c604a916-8764-4324-e38a-2fe0e82aedde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization with NLTK with correct pos tags:\n",
      "\n",
      "playing: play\n",
      "played: play\n",
      "plays: play\n",
      "play: play\n",
      "running: run\n",
      "ran: run\n",
      "runs: run\n",
      "run: run\n"
     ]
    }
   ],
   "source": [
    "print('Lemmatization with NLTK with correct pos tags:\\n')\n",
    "for word in words_to_stem:\n",
    "  print(f'{word}: {lemmatizer.lemmatize(word, pos=wordnet.VERB)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sozGhW_i17a4"
   },
   "source": [
    "Conveniently for us, Spacy does POS tagging and other necessary preprocessing for lemmatization, and we can get all the lemmas with only one command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OgE3DK8c1-mf",
    "outputId": "15e3e570-c0a4-4474-9a86-1705812f7557"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization with Spacy:\n",
      "\n",
      "playing: play\n",
      "played: play\n",
      "plays: play\n",
      "play: play\n",
      "running: run\n",
      "ran: run\n",
      "runs: run\n",
      "run: run\n"
     ]
    }
   ],
   "source": [
    "print('Lemmatization with Spacy:\\n')\n",
    "for word in words_to_stem: \n",
    "  doc = nlp(word)\n",
    "  print(f'{word}: {doc[0].lemma_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJbOxCEx2B0h"
   },
   "source": [
    "We can also see how our sentences from the previous exercise look after stemming and lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dqqldlMclvom",
    "outputId": "950592ed-1b37-44ef-893d-d3316cd2447e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK stemming:\n",
      "\n",
      "Original sentence:\n",
      "[['Dr.', 'Ford', 'did', 'not', 'ask', 'Col.', 'Mustard', 'the', 'name', 'of', 'Mr.', 'Smith', \"'s\", 'dog', '.']]\n",
      "Stemmed sentence:\n",
      "[['dr.', 'ford', 'did', 'not', 'ask', 'col.', 'mustard', 'the', 'name', 'of', 'mr.', 'smith', \"'s\", 'dog', '.']]\n",
      "\n",
      "------\n",
      "\n",
      "Original sentence:\n",
      "[['``', 'What', 'is', 'all', 'the', 'fuss', 'about', '?', \"''\"], ['asked', 'Mr.', 'Peters', '.']]\n",
      "Stemmed sentence:\n",
      "[['``', 'what', 'is', 'all', 'the', 'fuss', 'about', '?', \"''\"], ['ask', 'mr.', 'peter', '.']]\n",
      "\n",
      "------\n",
      "\n",
      "Original sentence:\n",
      "[['This', 'full-time', 'student', 'is', \"n't\", 'living', 'in', 'on-campus', 'housing', ',', 'and', 'she', \"'s\", 'not', 'wanting', 'to', 'visit', \"Hawai'i\", '.']]\n",
      "Stemmed sentence:\n",
      "[['thi', 'full-tim', 'student', 'is', \"n't\", 'live', 'in', 'on-campu', 'hous', ',', 'and', 'she', \"'s\", 'not', 'want', 'to', 'visit', \"hawai'i\", '.']]\n",
      "\n",
      "------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"NLTK stemming:\\n\")\n",
    "for sent in difficult_sents:\n",
    "  nltk_sents = [word_tokenize(s) for s in sent_tokenize(sent)]\n",
    "  print(f'Original sentence:\\n{nltk_sents}')\n",
    "  nltk_stems = []\n",
    "  for sent in nltk_sents:\n",
    "    stemmed_sent = []\n",
    "    for token in sent:\n",
    "      stemmed_sent.append(stemmer.stem(token))\n",
    "    nltk_stems.append(stemmed_sent)\n",
    "  print(f'Stemmed sentence:\\n{nltk_stems}')\n",
    "  print('\\n------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fg4UX4632Hvy"
   },
   "source": [
    "\n",
    "We can see the NLTK stemmer also puts all the words to lowercase which is another part of normalization. Also, we can also see some artifacts with the stemming like thi, full-tim, on-campu.\n",
    "\n",
    "Let's now see the lemmatized sentence from Spacy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xms8YUTe2HU7",
    "outputId": "0264070c-e15d-48c5-fe98-41fa56981862"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy lemmatization:\n",
      "\n",
      "Original sentence:\n",
      "['Dr.', 'Ford', 'did', 'not', 'ask', 'Col', '.', 'Mustard', 'the', 'name', 'of', 'Mr.', 'Smith', \"'s\", 'dog', '.']\n",
      "Lemmatized sentence:\n",
      "['Dr.', 'Ford', 'do', 'not', 'ask', 'Col', '.', 'Mustard', 'the', 'name', 'of', 'Mr.', 'Smith', \"'s\", 'dog', '.']\n",
      "\n",
      "------\n",
      "\n",
      "Original sentence:\n",
      "['\"', 'What', 'is', 'all', 'the', 'fuss', 'about', '?', '\"', 'asked', 'Mr.', 'Peters', '.']\n",
      "Lemmatized sentence:\n",
      "['\"', 'what', 'be', 'all', 'the', 'fuss', 'about', '?', '\"', 'ask', 'Mr.', 'Peters', '.']\n",
      "\n",
      "------\n",
      "\n",
      "Original sentence:\n",
      "['This', 'full', '-', 'time', 'student', 'is', \"n't\", 'living', 'in', 'on', '-', 'campus', 'housing', ',', 'and', 'she', \"'s\", 'not', 'wanting', 'to', 'visit', \"Hawai'i\", '.']\n",
      "Lemmatized sentence:\n",
      "['this', 'full', '-', 'time', 'student', 'be', 'not', 'live', 'in', 'on', '-', 'campus', 'housing', ',', 'and', '-PRON-', 'be', 'not', 'want', 'to', 'visit', \"Hawai'i\", '.']\n",
      "\n",
      "------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Spacy lemmatization:\\n\")\n",
    "for sent in difficult_sents:\n",
    "    doc = nlp(sent) \n",
    "    print(f'Original sentence:\\n{[token.text  for s in doc.sents for token in s]}')\n",
    "    print(f'Lemmatized sentence:\\n{[token.lemma_  for s in doc.sents for token in s]}')\n",
    "    print('\\n------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2-OiQaj2NjH"
   },
   "source": [
    "\n",
    "With lemmatization, the results look better: *did* trasformed to *do*, as well as *is* and *'s* to *be*. Another good thing is that in the first sentence *'s* in *Smith's dog* stayed as *'s* which is important because in this case it is not a contraction from the verb *is*.\n",
    "\n",
    "Another parts for the normalization include:\n",
    "\n",
    "* Removing the punctuation\n",
    "* Removing whitespace\n",
    "* Removing numbers or converting them into text\n",
    "* Removing stop words\n",
    "* etc\n",
    "\n",
    "Finally, we can look a bit more into the stop words. Stopwords are words such as a, an, the, in, at, and so on that occur frequently in text corpora\n",
    "and do not carry a lot of information in most contexts. These words, in general, are required\n",
    "for the completion of sentences and making them grammatically sound. They are often the\n",
    "most common words in a language and can be filtered out in most NLP tasks, and\n",
    "consequently help in reducing the vocabulary or search space.  However, the stop list can be modified to fit a specific task.\n",
    "\n",
    "Both NLTK and Spacy have built-in lists for stop words, however, you are free to find it anywhere else on the internet or even compose your own list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qv81sHf22Udk",
    "outputId": "4a2fdc3c-fdb3-46d7-eca1-7b25cc7733c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words for English from NLTK:\n",
      "\n",
      "{\"hasn't\", 'very', 'off', 'her', 'of', 'wasn', 'against', 'so', 'who', 'up', 'i', 'these', 'hadn', \"wasn't\", \"mightn't\", 'there', 'are', 'being', 'hers', 'further', 'both', 'while', 'those', \"don't\", 'before', \"didn't\", 'aren', 'or', 'shouldn', 'whom', 'this', 'yourselves', 'into', 'such', 'any', 'out', 'what', 'to', 'theirs', 'be', 're', 'as', 'just', 'then', 'yours', 'their', 'and', 'between', 'again', \"you've\", 'because', 'in', \"should've\", 'by', \"that'll\", \"weren't\", 'too', 'during', 'ourselves', 'mightn', 'your', 'won', 'how', 'needn', 'them', 'only', 'myself', 'each', 'wouldn', 'yourself', 'which', 'him', 'where', 'from', 'been', 'most', \"you're\", 'its', 'we', \"won't\", 'more', 'doing', 'on', 's', 'am', 'himself', 'some', 'have', 'all', 'a', 'y', 'mustn', 'for', 'the', 'had', 'doesn', 'ours', 'hasn', \"couldn't\", 'o', 'our', 'other', 'than', 'was', 'is', 'own', 'after', 't', 'until', 'should', 'few', 'were', 'ma', \"you'd\", 'itself', 'can', 'don', 'why', 'didn', \"hadn't\", 'you', \"she's\", \"doesn't\", 'when', 'haven', 'shan', 'once', 'an', \"shan't\", 'here', 'having', \"needn't\", \"isn't\", 'd', \"you'll\", 'same', 'if', 'above', 'does', 'couldn', 'his', 'will', 'herself', 'no', 'about', 'down', 'that', 'below', 'll', 've', 'my', 'over', 'themselves', 'they', 'do', 'with', 'me', 'nor', 'has', 'isn', 'at', \"it's\", 'she', 'ain', 'did', 'it', \"aren't\", 'but', 'm', 'he', 'under', \"mustn't\", \"haven't\", 'weren', 'not', \"shouldn't\", 'through', 'now', \"wouldn't\"}\n"
     ]
    }
   ],
   "source": [
    "print('Stop words for English from NLTK:\\n')\n",
    "\n",
    "nltk_stopwords = set(stopwords.words('english')) \n",
    "print(nltk_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CzMtlhlx2XCL",
    "outputId": "a6f26bae-71b4-4d9d-9bc6-9f4bf1c9bfc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words for English from Spacy:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\"'d\",\n",
       " \"'ll\",\n",
       " \"'m\",\n",
       " \"'re\",\n",
       " \"'s\",\n",
       " \"'ve\",\n",
       " 'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amount',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'are',\n",
       " 'around',\n",
       " 'as',\n",
       " 'at',\n",
       " 'back',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'both',\n",
       " 'bottom',\n",
       " 'but',\n",
       " 'by',\n",
       " 'ca',\n",
       " 'call',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'could',\n",
       " 'did',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doing',\n",
       " 'done',\n",
       " 'down',\n",
       " 'due',\n",
       " 'during',\n",
       " 'each',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'eleven',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'empty',\n",
       " 'enough',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'except',\n",
       " 'few',\n",
       " 'fifteen',\n",
       " 'fifty',\n",
       " 'first',\n",
       " 'five',\n",
       " 'for',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forty',\n",
       " 'four',\n",
       " 'from',\n",
       " 'front',\n",
       " 'full',\n",
       " 'further',\n",
       " 'get',\n",
       " 'give',\n",
       " 'go',\n",
       " 'had',\n",
       " 'has',\n",
       " 'have',\n",
       " 'he',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'however',\n",
       " 'hundred',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'indeed',\n",
       " 'into',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'keep',\n",
       " 'last',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'made',\n",
       " 'make',\n",
       " 'many',\n",
       " 'may',\n",
       " 'me',\n",
       " 'meanwhile',\n",
       " 'might',\n",
       " 'mine',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'move',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " \"n't\",\n",
       " 'name',\n",
       " 'namely',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'n‘t',\n",
       " 'n’t',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'part',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'please',\n",
       " 'put',\n",
       " 'quite',\n",
       " 'rather',\n",
       " 're',\n",
       " 'really',\n",
       " 'regarding',\n",
       " 'same',\n",
       " 'say',\n",
       " 'see',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'serious',\n",
       " 'several',\n",
       " 'she',\n",
       " 'should',\n",
       " 'show',\n",
       " 'side',\n",
       " 'since',\n",
       " 'six',\n",
       " 'sixty',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhere',\n",
       " 'still',\n",
       " 'such',\n",
       " 'take',\n",
       " 'ten',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'they',\n",
       " 'third',\n",
       " 'this',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'together',\n",
       " 'too',\n",
       " 'top',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'twelve',\n",
       " 'twenty',\n",
       " 'two',\n",
       " 'under',\n",
       " 'unless',\n",
       " 'until',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'used',\n",
       " 'using',\n",
       " 'various',\n",
       " 'very',\n",
       " 'via',\n",
       " 'was',\n",
       " 'we',\n",
       " 'well',\n",
       " 'were',\n",
       " 'what',\n",
       " 'whatever',\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'whither',\n",
       " 'who',\n",
       " 'whoever',\n",
       " 'whole',\n",
       " 'whom',\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'would',\n",
       " 'yet',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " '‘d',\n",
       " '‘ll',\n",
       " '‘m',\n",
       " '‘re',\n",
       " '‘s',\n",
       " '‘ve',\n",
       " '’d',\n",
       " '’ll',\n",
       " '’m',\n",
       " '’re',\n",
       " '’s',\n",
       " '’ve'}"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Stop words for English from Spacy:\\n')\n",
    "nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ThXpnen2aFD"
   },
   "source": [
    "Finally, we can see how our sentences look with the stop words removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mtnAvySDnWIj",
    "outputId": "dddbedd5-b986-480d-f57a-0bea59d1ea2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK stemming and stop words:\n",
      "\n",
      "Original sentence:\n",
      "[['Dr.', 'Ford', 'did', 'not', 'ask', 'Col.', 'Mustard', 'the', 'name', 'of', 'Mr.', 'Smith', \"'s\", 'dog', '.']]\n",
      "Stemmed sentence:\n",
      "[['dr.', 'ford', 'did', 'not', 'ask', 'col.', 'mustard', 'the', 'name', 'of', 'mr.', 'smith', \"'s\", 'dog', '.']]\n",
      "Stemmed sentence without stop words:\n",
      "['dr.', 'ford', 'ask', 'col.', 'mustard', 'name', 'mr.', 'smith', \"'s\", 'dog', '.']\n",
      "\n",
      "------\n",
      "\n",
      "Original sentence:\n",
      "[['``', 'What', 'is', 'all', 'the', 'fuss', 'about', '?', \"''\"], ['asked', 'Mr.', 'Peters', '.']]\n",
      "Stemmed sentence:\n",
      "[['``', 'what', 'is', 'all', 'the', 'fuss', 'about', '?', \"''\"], ['ask', 'mr.', 'peter', '.']]\n",
      "Stemmed sentence without stop words:\n",
      "['``', 'fuss', '?', \"''\", 'ask', 'mr.', 'peter', '.']\n",
      "\n",
      "------\n",
      "\n",
      "Original sentence:\n",
      "[['This', 'full-time', 'student', 'is', \"n't\", 'living', 'in', 'on-campus', 'housing', ',', 'and', 'she', \"'s\", 'not', 'wanting', 'to', 'visit', \"Hawai'i\", '.']]\n",
      "Stemmed sentence:\n",
      "[['thi', 'full-tim', 'student', 'is', \"n't\", 'live', 'in', 'on-campu', 'hous', ',', 'and', 'she', \"'s\", 'not', 'want', 'to', 'visit', \"hawai'i\", '.']]\n",
      "Stemmed sentence without stop words:\n",
      "['thi', 'full-tim', 'student', \"n't\", 'live', 'on-campu', 'hous', ',', \"'s\", 'want', 'visit', \"hawai'i\", '.']\n",
      "\n",
      "------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"NLTK stemming and stop words:\\n\")\n",
    "for sent in difficult_sents:\n",
    "    nltk_sents = [word_tokenize(s) for s in sent_tokenize(sent)]\n",
    "    print(f'Original sentence:\\n{nltk_sents}')\n",
    "    nltk_stems = []\n",
    "    nltk_no_stop = []\n",
    "    for sent in nltk_sents:\n",
    "        stemmed_sent = []\n",
    "        for token in sent:\n",
    "            stemmed_token = stemmer.stem(token)\n",
    "            if stemmed_token not in nltk_stopwords:\n",
    "                nltk_no_stop.append(stemmed_token)\n",
    "            stemmed_sent.append(stemmed_token)\n",
    "        nltk_stems.append(stemmed_sent)\n",
    "    print(f'Stemmed sentence:\\n{nltk_stems}')\n",
    "    print(f'Stemmed sentence without stop words:\\n{nltk_no_stop}')\n",
    "    print('\\n------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R15rRiyw2i9s",
    "outputId": "e3a5ae1a-d3dc-4ca9-e801-ef53ff21b089"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy lemmatization and stop words:\n",
      "\n",
      "Original sentence:\n",
      "['Dr.', 'Ford', 'did', 'not', 'ask', 'Col', '.', 'Mustard', 'the', 'name', 'of', 'Mr.', 'Smith', \"'s\", 'dog', '.']\n",
      "Lemmatized sentence:\n",
      "['Dr.', 'Ford', 'do', 'not', 'ask', 'Col', '.', 'Mustard', 'the', 'name', 'of', 'Mr.', 'Smith', \"'s\", 'dog', '.']\n",
      "Lemmatized sentence without stop words:\n",
      "['Dr.', 'Ford', 'ask', 'Col', '.', 'Mustard', 'Mr.', 'Smith', 'dog', '.']\n",
      "\n",
      "------\n",
      "\n",
      "Original sentence:\n",
      "['\"', 'What', 'is', 'all', 'the', 'fuss', 'about', '?', '\"', 'asked', 'Mr.', 'Peters', '.']\n",
      "Lemmatized sentence:\n",
      "['\"', 'what', 'be', 'all', 'the', 'fuss', 'about', '?', '\"', 'ask', 'Mr.', 'Peters', '.']\n",
      "Lemmatized sentence without stop words:\n",
      "['\"', 'fuss', '?', '\"', 'ask', 'Mr.', 'Peters', '.']\n",
      "\n",
      "------\n",
      "\n",
      "Original sentence:\n",
      "['This', 'full', '-', 'time', 'student', 'is', \"n't\", 'living', 'in', 'on', '-', 'campus', 'housing', ',', 'and', 'she', \"'s\", 'not', 'wanting', 'to', 'visit', \"Hawai'i\", '.']\n",
      "Lemmatized sentence:\n",
      "['this', 'full', '-', 'time', 'student', 'be', 'not', 'live', 'in', 'on', '-', 'campus', 'housing', ',', 'and', '-PRON-', 'be', 'not', 'want', 'to', 'visit', \"Hawai'i\", '.']\n",
      "Lemmatized sentence without stop words:\n",
      "['-', 'time', 'student', 'live', '-', 'campus', 'housing', ',', '-PRON-', 'want', 'visit', \"Hawai'i\", '.']\n",
      "\n",
      "------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Spacy lemmatization and stop words:\\n\")\n",
    "for sent in difficult_sents:\n",
    "    doc = nlp(sent) \n",
    "    print(f'Original sentence:\\n{[token.text for s in doc.sents for token in s]}') \n",
    "    print(f'Lemmatized sentence:\\n{[token.lemma_ for s in doc.sents for token in s]}')\n",
    "    print(f'Lemmatized sentence without stop words:\\n{[token.lemma_ for s in doc.sents for token in s if token.lemma_ not in nlp.Defaults.stop_words]}') \n",
    "    print('\\n------\\n')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lab1_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
